{
    "t5-vicuna-3b": {
        "category": "<10B",
        "display_name": "T5 Vicuna",
        "thumb": "https://i.ibb.co/4W7n78b/chansung-vector-logo-of-collective-intelligence-of-cute-llamas-3ef46884-72e6-44da-b88a-e831e5fee747.png",
        "parameters": "3 Billion",
        "hub(base)": "lmsys/fastchat-t5-3b-v1.0",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/t5_vicuna.yaml",
        "desc": "This model is based on [Flan-T5-XL](https://huggingface.co/google/flan-t5-xl) from google, and it is fine-tuned on [ShareGPT](https://sharegpt.com/) dataset curated by [LMSYS](https://lmsys.org/) in [Vicuna style](https://github.com/lm-sys/FastChat)"
    },
    "replit-3b": {
        "category": "<10B",
        "display_name": "Replit Instruct",
        "thumb": "https://i.ibb.co/BrKCKYq/replit.png",
        "parameters": "3 Billion",
        "hub(base)": "teknium/Replit-v1-CodeInstruct-3B",
        "hub(ckpt)": "N/A",
        "desc": ""
    },
    "camel-5b": {
        "category": "<10B",
        "display_name": "Camel",
        "thumb": "https://i.ibb.co/qD5HN9T/camel-removebg-preview.png",
        "parameters": "5 Billion",
        "hub(base)": "Writer/camel-5b-hf",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/camel.yaml",
        "desc": "Derived from the foundational architecture of [Palmyra-Base](https://huggingface.co/Writer/palmyra-base), Camel-5b is fine-tuned on an extensive dataset of approximately 70,000 instruction-response records. These records are generated by our dedicated [Writer](https://writer.com/) Linguist team, who possess considerable expertise in language modeling and fine-tuning techniques."
    },
    "mpt-7b": {
        "category": "<10B",
        "display_name": "MPT",
        "thumb": "https://i.ibb.co/DwN44Z9/mpt.png",
        "parameters": "7 Billion",
        "hub(base)": "mosaicml/mpt-7b-chat",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "[MPT-7B-Chat](https://huggingface.co/spaces/mosaicml/mpt-7b) is a chatbot-like model for dialogue generation. It was built by finetuning MPT-7B on the [ShareGPT-Vicuna](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna), [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3), [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca), [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf), and [Evol-Instruct](https://huggingface.co/datasets/victor123/evol_instruct_70k) datasets."
    },
    "redpajama-7b": {
        "category": "<10B",
        "display_name": "RedPajama",
        "thumb": "https://i.ibb.co/NNB6qPj/redpajama.png",
        "parameters": "7 Billion",
        "hub(base)": "togethercomputer/RedPajama-INCITE-Chat-7B-v0.1",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/redpajama.yaml",
        "desc": "RedPajama-INCITE-Chat-7B-v0.1 was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. It is fine-tuned on [OASST1](https://huggingface.co/datasets/OpenAssistant/oasst1) and [Dolly2](https://huggingface.co/datasets/databricks/databricks-dolly-15k) to enhance chatting ability."
    },    
    "alpaca-lora-7b": {
        "category": "<10B",
        "display_name": "Alpaca-LoRA",
        "thumb": "https://i.ibb.co/z89FTz2/alpaca-lora.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Alpaca-LoRA-7B-elina",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "Alpaca LoRA is LLaMA 7B model fine-tuned on Alpaca dataset which is originally introduced by Standford. The dataset in this model was cleaned and re-generated by GPT4 by the community."
    },
    "alpaca-lora-13b": {
        "category": "<20B",
        "display_name": "Alpaca-LoRA",
        "thumb": "https://i.ibb.co/z89FTz2/alpaca-lora.png",
        "parameters": "13 Billion",
        "hub(base)": "elinas/llama-13b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Alpaca-LoRA-13B-elina",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "Alpaca LoRA is LLaMA 13B model fine-tuned on Alpaca dataset which is originally introduced by Standford. The dataset in this model was cleaned and re-generated by GPT4 by the community."
    },
    "koalpaca": {
        "category": "<20B",
        "display_name": "KoAlpaca",
        "thumb": "https://i.ibb.co/hF9NL7r/koalpaca.png",
        "parameters": "12.8 Billion",
        "hub(base)": "beomi/KoAlpaca-Polyglot-12.8B",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/koalpaca.yaml",
        "desc": "KoAlpaca is ko-polyglot 12.8B model fine-tuned on Alpaca dataset which is originally introduced by Standford. The dataset in this model was cleaned and translated into Korean using DeepL by the model creator."
    },
    "kullm": {
        "category": "<20B",
        "display_name": "kullm",
        "thumb": "https://i.ibb.co/6ZFqk4J/kullm.png",
        "parameters": "12.8 Billion",
        "hub(base)": "nlpai-lab/kullm-polyglot-12.8b-v2",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/koalpaca.yaml",
        "desc": "KoAlpaca is ko-polyglot 12.8B model fine-tuned on [GTP4ALL](https://github.com/nomic-ai/gpt4all), [Vicuna](https://github.com/lm-sys/FastChat), and [Dolly](https://github.com/databrickslabs/dolly) datasets. The datasets are translated into Korean using DeepL by the model creator."
    },    
    "stablelm-7b": {
        "category": "<10B",
        "display_name": "StableLM",
        "thumb": "https://i.ibb.co/d2pd5wk/stable-LM-cropped.png",
        "parameters": "7 Billion",
        "hub(base)": "stabilityai/stablelm-tuned-alpha-7b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/stablelm.yaml",
        "desc": "StableLM 7B is a 7B parameter decoder-only language model built on top of the [StableLM-Base-Alpha](https://huggingface.co/stabilityai/stablelm-base-alpha-7b) model and further fine-tuned on various chat and instruction-following datasets including [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca), [GPT4ALL](https://huggingface.co/datasets/nomic-ai/gpt4all_prompt_generations), [Anthropic HH](https://huggingface.co/datasets/Dahoas/full-hh-rlhf), [Dolly](https://github.com/databrickslabs/dolly), and [ShareGPT](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna)."
    },
    "os-stablelm-7b": {
        "category": "<10B",
        "display_name": "OA+StableLM",
        "thumb": "https://i.ibb.co/WszrtVV/stablelm-oasst1.png",
        "parameters": "7 Billion",
        "hub(base)": "OpenAssistant/stablelm-7b-sft-v7-epoch-3",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/stablelm.yaml",
        "desc": "This is the 7th iteration English supervised-fine-tuning (SFT) model of the [Open-Assistant](https://github.com/LAION-AI/Open-Assistant) project. It is based on a StableLM 7B that was fine-tuned on human demonstrations of assistant conversations collected through the [https://open-assistant.io/](https://open-assistant.io/) human feedback web app before April 12, 2023. The datasets include [OASST](https://huggingface.co/datasets/OpenAssistant/oasst1), [Vicuna](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna), [Dolly](https://github.com/databrickslabs/dolly), grade school math instructions, and [code alpaca](https://github.com/sahil280114/codealpaca)."
    },
    "stackllama-7b": {
        "category": "<10B",
        "display_name": "StackLLaMA",
        "thumb": "https://i.ibb.co/Q9vLcYm/tuxpi-com-1682256296-removebg-preview.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "trl-lib/llama-7b-se-rl-peft",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": ""
    },
    "flan-3b": {
        "category": "<10B",
        "display_name": "Flan-XL",
        "thumb": "https://i.ibb.co/yBTk5bv/flan.png",
        "parameters": "3 Billion",
        "hub(base)": "declare-lab/flan-alpaca-xl",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/flan.yaml",
        "desc": "flan-alpaca-xl is [Flan-T5-XL](https://huggingface.co/google/flan-t5-xl) 3B fine-tuned on [Flan](https://github.com/google-research/FLAN) and [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) datasets."
    },
    "baize-7b": {
        "category": "<10B",
        "display_name": "Baize",
        "thumb": "https://i.ibb.co/j5VpHb0/baize.png",
        "parameters": "7 Billion",
        "hub(base)": "project-baize/baize-v2-7b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/baize.yaml",
        "desc": "This model is a 7B Baize-v2, trained with supervised fine-tuning (SFT) and self-distillation with feedback (SDF). This checkpoint has been merged with LLaMA 7B so it's ready for use."
    },
    "gpt4-alpaca-7b": {
        "category": "<10B",
        "display_name": "GPT4-Alpaca-LoRA",
        "thumb": "https://i.ibb.co/qDz3HCG/chansung-vector-logo-of-alpaca-made-out-of-machines-Side-shot-39b27595-8202-48a6-97d1-266a745b2a29-r.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/AlpacaGPT4-LoRA-7B-elina",
        "default_gen_config": "configs/response_configs/gpt4_alpaca.yaml",
        "desc": "GPT4 Alpaca LoRA is LLaMA 7B model fine-tuned on GPT4 generated instruction dataset."
    },
    "vicuna-7b": {
        "category": "<10B",
        "display_name": "Vicuna",
        "thumb": "https://i.ibb.co/vqPDrPQ/vicuna.png",
        "parameters": "7 Billion",
        "hub(base)": "LLMs/Vicuna-7b-v1.1",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "Vicuna 7B is an open-source chatbot trained by fine-tuning LLaMA 7B on user-shared conversations collected from ShareGPT. It is an auto-regressive language model, based on the transformer architecture."
    },
    "baize-13b": {
        "category": "<20B",
        "display_name": "Baize",
        "thumb": "https://i.ibb.co/j5VpHb0/baize.png",
        "parameters": "13 Billion",
        "hub(base)": "project-baize/baize-v2-13b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/baize.yaml",
        "desc": "This model is a 13B Baize-v2, trained with supervised fine-tuning (SFT) and self-distillation with feedback (SDF). This checkpoint has been merged with LLaMA 7B so it's ready for use."
    },    
    "vicuna-13b": {
        "category": "<20B",
        "display_name": "Vicuna",
        "thumb": "https://i.ibb.co/vqPDrPQ/vicuna.png",
        "parameters": "13 Billion",
        "hub(base)": "LLMs/Vicuna-13b-v1.1",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "Vicuna 13B is an open-source chatbot trained by fine-tuning LLaMA 13B on user-shared conversations collected from ShareGPT. It is an auto-regressive language model, based on the transformer architecture."
    },    
    "gpt4-alpaca-13b": {
        "category": "<20B",
        "display_name": "GPT4-Alpaca-LoRA",
        "thumb": "https://i.ibb.co/qDz3HCG/chansung-vector-logo-of-alpaca-made-out-of-machines-Side-shot-39b27595-8202-48a6-97d1-266a745b2a29-r.png",
        "parameters": "13 Billion",
        "hub(base)": "elinas/llama-13b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/AlpacaGPT4-LoRA-13B-elina",
        "default_gen_config": "configs/response_configs/gpt4_alpaca.yaml",
        "desc": "GPT4 Alpaca LoRA is LLaMA 13B model fine-tuned on GPT4 generated instruction dataset."
    },    
    "flan-11b": {
        "category": "<20B",
        "display_name": "Flan-XXL",
        "thumb": "https://i.ibb.co/yBTk5bv/flan.png",
        "parameters": "11 Billion",
        "hub(base)": "declare-lab/flan-alpaca-xxl",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/flan.yaml",
        "desc": "flan-alpaca-xxl is [Flan-T5-XXL](https://huggingface.co/google/flan-t5-xl) 11B fine-tuned on [Flan](https://github.com/google-research/FLAN) and [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) datasets."
    },
    "stable-vicuna-13b": {
        "category": "<20B",
        "display_name": "Stable-Vicuna",        
        "thumb": "https://i.ibb.co/b6Vv6Jh/sv.png",
        "parameters": "13 Billion",
        "hub(base)": "LLMs/Stable-Vicuna-13B",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/stablelm.yaml",
        "desc": "StableVicuna-13B is a [Vicuna-13B v0](https://huggingface.co/lmsys/vicuna-13b-delta-v0) model fine-tuned using reinforcement learning from human feedback (RLHF) via Proximal Policy Optimization (PPO) on various conversational and instructional datasets."
    },
    "camel-20b": {
        "category": "<30B",
        "display_name": "Camel",
        "thumb": "https://i.ibb.co/qD5HN9T/camel-removebg-preview.png",
        "parameters": "20 Billion",
        "hub(base)": "Writer/camel-20b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/camel.yaml",
        "desc": "Derived from the foundational architecture of [Palmyra-Base](https://huggingface.co/Writer/palmyra-base), Camel-20b is fine-tuned on an extensive dataset of approximately 70,000 instruction-response records. These records are generated by our dedicated [Writer](https://writer.com/) Linguist team, who possess considerable expertise in language modeling and fine-tuning techniques."
    },
    "starchat-15b": {
        "category": "<20B",
        "display_name": "StarChat",
        "thumb": "https://i.ibb.co/QjPP0Vv/starcoder.png",
        "parameters": "15.5 Billion",
        "hub(base)": "HuggingFaceH4/starchat-alpha",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/starchat.yaml",
        "desc": "StarChat is a series of language models that are fine-tuned from StarCoder to act as helpful coding assistants. StarChat Alpha is the first of these models, and as an alpha release is only intended for educational or research purpopses."
    },
    "llama-deus-7b": {
        "category": "<10B",
        "display_name": "LLaMA Deus",        
        "thumb": "https://i.ibb.co/4mH9LRQ/llama-deus.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "teknium/llama-deus-7b-v3-lora",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "LLaMA Deus is LLaMA 7B model fine-tuned on [GPTeacher](https://github.com/teknium1/GPTeacher)(General Instruct, Code Instruct, Roleplay Instruct, Roleplay V2 Instruct), GPT4-LLM Uncensored + Unnatural Instructions, [WizardLM Uncensored](https://huggingface.co/datasets/ehartford/wizard_vicuna_70k_unfiltered)"
    },
    "evolinstruct-vicuna-7b": {
        "category": "<10B",
        "display_name": "EvolInstruct Vicuna",        
        "thumb": "https://i.ibb.co/xHDRjLS/evol-vicuna.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Vicuna-LoRA-EvolInstruct-7B",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "This is LLaMA 7B model fine-tuned on [WizardLM's EvolInstruct](https://github.com/nlpxucan/WizardLM) dataset re-organized into conversational format in Vicuna style."
    },
    "evolinstruct-vicuna-13b": {
        "category": "<20B",
        "display_name": "EvolInstruct Vicuna",        
        "thumb": "https://i.ibb.co/xHDRjLS/evol-vicuna.png",
        "parameters": "13 Billion",
        "hub(base)": "elinas/llama-13b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Vicuna-LoRA-EvolInstruct-13B",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "This is LLaMA 13B model fine-tuned on [WizardLM's EvolInstruct](https://github.com/nlpxucan/WizardLM) dataset re-organized into conversational format in Vicuna style."
    },
    "alpacoom-7b": {
        "category": "<10B",
        "display_name": "Alpacoom",        
        "thumb": "https://huggingface.co/mrm8488/Alpacoom/resolve/main/alpacoom_logo__1___1___1_-removebg-preview.png",
        "parameters": "7 Billion",
        "hub(base)": "bigscience/bloom-7b1",
        "hub(ckpt)": "mrm8488/Alpacoom",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "This adapter was created with the PEFT library and allowed the base model [BigScience/BLOOM 7B1](https://huggingface.co/bigscience/bloom-7b1) to be fine-tuned on the Stanford's Alpaca Dataset by using the method LoRA."
    },
    "guanaco-7b": {
        "category": "<10B",
        "display_name": "Guanaco",        
        "thumb": "https://i.ibb.co/DWWsZn7/guanaco.png",
        "parameters": "7 Billion",
        "hub(base)": "decapoda-research/llama-7b-hf",
        "hub(ckpt)": "timdettmers/guanaco-7b",
        "default_gen_config": "configs/response_configs/guanaco.yaml",
        "desc": "LLaMA 7B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora) paper."
    },
    "guanaco-13b": {
        "category": "<20B",
        "display_name": "Guanaco",        
        "thumb": "https://i.ibb.co/DWWsZn7/guanaco.png",
        "parameters": "13 Billion",
        "hub(base)": "decapoda-research/llama-13b-hf",
        "hub(ckpt)": "timdettmers/guanaco-13b",
        "default_gen_config": "configs/response_configs/guanaco.yaml",
        "desc": "LLaMA 13B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora) paper."
    },
    "guanaco-33b": {
        "category": "<40B",
        "display_name": "Guanaco",        
        "thumb": "https://i.ibb.co/DWWsZn7/guanaco.png",
        "parameters": "33 Billion",
        "hub(base)": "timdettmers/guanaco-33b-merged",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/guanaco.yaml",
        "desc": "LLaMA 30B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora) paper."
    },
    "falcon-7b": {
        "category": "<10B",
        "display_name": "Falcon",
        "thumb": "https://i.ibb.co/86yNWwG/falcon.png",
        "parameters": "7 Billion",
        "hub(base)": "tiiuae/falcon-7b-instruct",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/falcon.yaml",
        "desc": "Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by [TII](https://www.tii.ae/) based on [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) and finetuned on a mixture of chat/instruct datasets including [Baize](https://github.com/project-baize/baize-chatbot), [GPT4All](https://github.com/nomic-ai/gpt4all), [GPTeacher](https://github.com/teknium1/GPTeacher), and [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)"
    },
    "falcon-40b": {
        "category": "<40B",
        "display_name": "Falcon",
        "thumb": "https://i.ibb.co/86yNWwG/falcon.png",
        "parameters": "40 Billion",
        "hub(base)": "tiiuae/falcon-40b-instruct",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/falcon.yaml",
        "desc": "Falcon-7B-Instruct is a 40B parameters causal decoder-only model built by [TII](https://www.tii.ae/) based on [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) and finetuned on a mixture of chat/instruct datasets including [Baize](https://github.com/project-baize/baize-chatbot), [GPT4All](https://github.com/nomic-ai/gpt4all), [GPTeacher](https://github.com/teknium1/GPTeacher), and [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)"
    },
    "wizard-falcon-7b": {
        "category": "<10B",
        "display_name": "Wizard Falcon",
        "thumb": "https://i.ibb.co/415s0D4/wizard-falcon.png",
        "parameters": "7 Billion",
        "hub(base)": "ehartford/WizardLM-Uncensored-Falcon-7b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/falcon.yaml",
        "desc": "This is WizardLM trained on top of tiiuae/falcon-7b, with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA. An uncensored model has no guardrails."
    },
    "wizard-falcon-40b": {
        "category": "<40B",
        "display_name": "Wizard Falcon",
        "thumb": "https://i.ibb.co/415s0D4/wizard-falcon.png",
        "parameters": "40 Billion",
        "hub(base)": "ehartford/WizardLM-Uncensored-Falcon-40b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/falcon.yaml",
        "desc": "This is WizardLM trained on top of tiiuae/falcon-40b, with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA. An uncensored model has no guardrails."
    },    
    "nous-hermes-13b": {
        "category": "<20B",
        "display_name": "Nous Hermes",
        "thumb": "https://i.ibb.co/sm8VgtL/nous-hermes.png",
        "parameters": "13 Billion",
        "hub(base)": "NousResearch/Nous-Hermes-13b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "The model is LLaMA 13B that was fine-tuned almost entirely on synthetic GPT-4 outputs. This includes data from diverse sources such as GPTeacher, the general, roleplay v1&2, code instruct datasets, Nous Instruct & PDACTL (unpublished), CodeAlpaca, Evol_Instruct Uncensored, GPT4-LLM, and Unnatural Instructions. Additional data inputs came from Camel-AI's Biology/Physics/Chemistry and Math Datasets, Airoboros' GPT-4 Dataset, and more from CodeAlpaca. The total volume of data encompassed over 300,000 instructions." 
    }
}
