{
    "t5-vicuna-3b": {
        "category": "<10B",
        "display_name": "T5 Vicuna",
        "thumb": "https://i.ibb.co/4W7n78b/chansung-vector-logo-of-collective-intelligence-of-cute-llamas-3ef46884-72e6-44da-b88a-e831e5fee747.png",
        "parameters": "3 Billion",
        "hub(base)": "lmsys/fastchat-t5-3b-v1.0",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/t5_vicuna.yaml",
        "desc": "This model is based on [Flan-T5-XL](https://huggingface.co/google/flan-t5-xl) from google, and it is fine-tuned on [ShareGPT](https://sharegpt.com/) dataset curated by [LMSYS](https://lmsys.org/) in [Vicuna style](https://github.com/lm-sys/FastChat)",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "replit-3b": {
        "category": "<10B",
        "display_name": "Replit Instruct",
        "thumb": "https://i.ibb.co/BrKCKYq/replit.png",
        "parameters": "3 Billion",
        "hub(base)": "teknium/Replit-v1-CodeInstruct-3B",
        "hub(ckpt)": "N/A",
        "desc": ""
    },
    "camel-5b": {
        "category": "<10B",
        "display_name": "Camel",
        "thumb": "https://i.ibb.co/qD5HN9T/camel-removebg-preview.png",
        "parameters": "5 Billion",
        "hub(base)": "Writer/camel-5b-hf",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/camel.yaml",
        "desc": "Derived from the foundational architecture of [Palmyra-Base](https://huggingface.co/Writer/palmyra-base), Camel-5b is fine-tuned on an extensive dataset of approximately 70,000 instruction-response records. These records are generated by our dedicated [Writer](https://writer.com/) Linguist team, who possess considerable expertise in language modeling and fine-tuning techniques.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "mpt-7b": {
        "category": "<10B",
        "display_name": "MPT",
        "thumb": "https://i.ibb.co/DwN44Z9/mpt.png",
        "parameters": "7 Billion",
        "hub(base)": "mosaicml/mpt-7b-chat",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "[MPT-7B-Chat](https://huggingface.co/spaces/mosaicml/mpt-7b) is a chatbot-like model for dialogue generation. It was built by finetuning MPT-7B on the [ShareGPT-Vicuna](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna), [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3), [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca), [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf), and [Evol-Instruct](https://huggingface.co/datasets/victor123/evol_instruct_70k) datasets.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "redpajama-7b": {
        "category": "<10B",
        "display_name": "RedPajama",
        "thumb": "https://i.ibb.co/NNB6qPj/redpajama.png",
        "parameters": "7 Billion",
        "hub(base)": "togethercomputer/RedPajama-INCITE-Chat-7B-v0.1",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/redpajama.yaml",
        "desc": "RedPajama-INCITE-Chat-7B-v0.1 was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. It is fine-tuned on [OASST1](https://huggingface.co/datasets/OpenAssistant/oasst1) and [Dolly2](https://huggingface.co/datasets/databricks/databricks-dolly-15k) to enhance chatting ability.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },    
    "alpaca-lora-7b": {
        "category": "<10B",
        "display_name": "Alpaca-LoRA",
        "thumb": "https://i.ibb.co/z89FTz2/alpaca-lora.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Alpaca-LoRA-7B-elina",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "Alpaca LoRA is LLaMA 7B model fine-tuned on Alpaca dataset which is originally introduced by Standford. The dataset in this model was cleaned and re-generated by GPT4 by the community.",
        "example1": [
            ["Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.", "Aloha! Have you ever dreamed of exploring the beautiful tropical paradise that is Hawai'i? If so, then it’s time for your journey of discovery to begin! I recently had the privilege of embarking on an epic adventure through one of America’s most exciting destinations—and here are some of my favorite memories from the experience.\n\nFrom the stunning sights of Honolulu like Diamond Head Crater to the breathtaking beauty of Maui’s Wailea coastline, there were plenty of jaw-dropping moments during this journey across five islands in fourteen days. My top pick would have to be our visit to the island of Kauai, home to iconic natural landmarks such as the Napali Coast, which was truly spectacular. The Na Pali Cliffs stretch over two thousand feet above sea level while providing impressive views of the Pacific Ocean below. It felt almost otherworldly seeing these cliffs rise straight up out of the ocean; I can hardly believe they exist in reality at all! Other memorable stops included Oahu, where we toured Pearl Harbor; Molokai with its lush greenery, waterfalls, and untamed nature; Lana'i with its unique flora and fauna; and Haleakala National Park, where we marveled at incredible vistas of the mountain range. We even managed to take part in traditional luaus, hiking volcanos, and snorkeling amidst colorful fishes and corals - needless to say, each day brought us new surprises and fresh adventures. With its amazingly diverse landscape and culture, Hawai'i offers visitors something truly special. This remarkable place has become a treasured memory for me and will remain forever close to heart."]
        ],
        "example2": [
            ["Can you help me write a resignation letter to my current employer, while leaving on good terms and expressing gratitude for the opportunities provided?", "Dear [Name of Employer],\n\nIt is with both sadness and excitement that I am writing this email. After much thoughtful consideration over the last few months, I have decided to pursue other career pathways which better align with my skills and ambitions going forward. As such, it brings me great joy to inform you today that I will be moving onto new opportunities in two weeks time.\n\nI would like to take this opportunity to thank all those who work here at XYZ Co., as they made me feel welcome and valued from day one. It has been an absolute privilege to learn so many new things under your guidance during these past several years - including the importance of teamwork, the value of integrity and how to approach problems with creativity. The experiences gained through working together in our daily tasks have helped shape my professional journey and become more effective communicator throughout. For all of this I cannot truly find words to properly convey my appreciation.\n\nOn behalf of myself and family, we wish every success for yourself, the company and everyone involved in making us happy during these times. We look back fondly upon our days spent together, and know you’ll continue making positive impacts within this world far into the future. Thank you again for everything!"]
        ],
        "example3": [
            ["What factors would you consider when designing an inclusive and accessible public transportation system?", "When designing a public transportation network, it is important to take into account accessibility for all users. This includes ensuring that the route networks have wheelchair ramps or lifts so people with disabilities can use them without difficulty; providing adequate bus stops along the way which are equipped with signage that displays information such as departure times and routes in order to reduce confusion amongst passengers who need assistance boarding or de-boarding; employment of friendly staff trained on customer service skills; provision of audio announcements of destination at each stop so visually impaired persons can better orient themselves within their surroundings; offering discounts for elderly individuals or those with special needs; allowing passengers with reduced mobility space to move around freely inside the vehicle; etc"]
        ],
        "example4": [
            ["Write a python program to print the first 10 Fibonacci numbers", "\"\"\"Prints out the first ten Fibonacci numbers.\nFibonacci sequence: (f(n+1)) = f(i) + f((n - i)); n=2; f(1)=1 ; Print(\"Fibonacci sequence for the next X elements :\") ; print(\"\\nFirst 10\nelements\\n\",sum([n*j/gcd(lcm(x),y)|x<=m|m<=(n+(5)), y <= x])for m, j, x, y in zip(range(1,len(data)+1),(3,2,1)))"]
        ]
    },
    "alpaca-lora-13b": {
        "category": "<20B",
        "display_name": "Alpaca-LoRA",
        "thumb": "https://i.ibb.co/z89FTz2/alpaca-lora.png",
        "parameters": "13 Billion",
        "hub(base)": "elinas/llama-13b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Alpaca-LoRA-13B-elina",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "Alpaca LoRA is LLaMA 13B model fine-tuned on Alpaca dataset which is originally introduced by Standford. The dataset in this model was cleaned and re-generated by GPT4 by the community.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "koalpaca": {
        "category": "<20B",
        "display_name": "KoAlpaca",
        "thumb": "https://i.ibb.co/hF9NL7r/koalpaca.png",
        "parameters": "12.8 Billion",
        "hub(base)": "beomi/KoAlpaca-Polyglot-12.8B",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/koalpaca.yaml",
        "desc": "KoAlpaca is ko-polyglot 12.8B model fine-tuned on Alpaca dataset which is originally introduced by Standford. The dataset in this model was cleaned and translated into Korean using DeepL by the model creator.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "kullm": {
        "category": "<20B",
        "display_name": "kullm",
        "thumb": "https://i.ibb.co/6ZFqk4J/kullm.png",
        "parameters": "12.8 Billion",
        "hub(base)": "nlpai-lab/kullm-polyglot-12.8b-v2",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/koalpaca.yaml",
        "desc": "KoAlpaca is ko-polyglot 12.8B model fine-tuned on [GTP4ALL](https://github.com/nomic-ai/gpt4all), [Vicuna](https://github.com/lm-sys/FastChat), and [Dolly](https://github.com/databrickslabs/dolly) datasets. The datasets are translated into Korean using DeepL by the model creator.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },    
    "stablelm-7b": {
        "category": "<10B",
        "display_name": "StableLM",
        "thumb": "https://i.ibb.co/d2pd5wk/stable-LM-cropped.png",
        "parameters": "7 Billion",
        "hub(base)": "stabilityai/stablelm-tuned-alpha-7b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/stablelm.yaml",
        "desc": "StableLM 7B is a 7B parameter decoder-only language model built on top of the [StableLM-Base-Alpha](https://huggingface.co/stabilityai/stablelm-base-alpha-7b) model and further fine-tuned on various chat and instruction-following datasets including [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca), [GPT4ALL](https://huggingface.co/datasets/nomic-ai/gpt4all_prompt_generations), [Anthropic HH](https://huggingface.co/datasets/Dahoas/full-hh-rlhf), [Dolly](https://github.com/databrickslabs/dolly), and [ShareGPT](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna).",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "os-stablelm-7b": {
        "category": "<10B",
        "display_name": "OA+StableLM",
        "thumb": "https://i.ibb.co/WszrtVV/stablelm-oasst1.png",
        "parameters": "7 Billion",
        "hub(base)": "OpenAssistant/stablelm-7b-sft-v7-epoch-3",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/stablelm.yaml",
        "desc": "This is the 7th iteration English supervised-fine-tuning (SFT) model of the [Open-Assistant](https://github.com/LAION-AI/Open-Assistant) project. It is based on a StableLM 7B that was fine-tuned on human demonstrations of assistant conversations collected through the [https://open-assistant.io/](https://open-assistant.io/) human feedback web app before April 12, 2023. The datasets include [OASST](https://huggingface.co/datasets/OpenAssistant/oasst1), [Vicuna](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna), [Dolly](https://github.com/databrickslabs/dolly), grade school math instructions, and [code alpaca](https://github.com/sahil280114/codealpaca).",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "stackllama-7b": {
        "category": "<10B",
        "display_name": "StackLLaMA",
        "thumb": "https://i.ibb.co/Q9vLcYm/tuxpi-com-1682256296-removebg-preview.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "trl-lib/llama-7b-se-rl-peft",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": ""
    },
    "flan-3b": {
        "category": "<10B",
        "display_name": "Flan-XL",
        "thumb": "https://i.ibb.co/yBTk5bv/flan.png",
        "parameters": "3 Billion",
        "hub(base)": "declare-lab/flan-alpaca-xl",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/flan.yaml",
        "desc": "flan-alpaca-xl is [Flan-T5-XL](https://huggingface.co/google/flan-t5-xl) 3B fine-tuned on [Flan](https://github.com/google-research/FLAN) and [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) datasets.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "baize-7b": {
        "category": "<10B",
        "display_name": "Baize",
        "thumb": "https://i.ibb.co/j5VpHb0/baize.png",
        "parameters": "7 Billion",
        "hub(base)": "project-baize/baize-v2-7b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/baize.yaml",
        "desc": "This model is a 7B Baize-v2, trained with supervised fine-tuning (SFT) and self-distillation with feedback (SDF). This checkpoint has been merged with LLaMA 7B so it's ready for use.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "gpt4-alpaca-7b": {
        "category": "<10B",
        "display_name": "GPT4-Alpaca-LoRA",
        "thumb": "https://i.ibb.co/qDz3HCG/chansung-vector-logo-of-alpaca-made-out-of-machines-Side-shot-39b27595-8202-48a6-97d1-266a745b2a29-r.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/AlpacaGPT4-LoRA-7B-elina",
        "default_gen_config": "configs/response_configs/gpt4_alpaca.yaml",
        "desc": "GPT4 Alpaca LoRA is LLaMA 7B model fine-tuned on GPT4 generated instruction dataset.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "vicuna-7b": {
        "category": "<10B",
        "display_name": "Vicuna",
        "thumb": "https://i.ibb.co/vqPDrPQ/vicuna.png",
        "parameters": "7 Billion",
        "hub(base)": "LLMs/Vicuna-7b-v1.1",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "Vicuna 7B is an open-source chatbot trained by fine-tuning LLaMA 7B on user-shared conversations collected from ShareGPT. It is an auto-regressive language model, based on the transformer architecture.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "baize-13b": {
        "category": "<20B",
        "display_name": "Baize",
        "thumb": "https://i.ibb.co/j5VpHb0/baize.png",
        "parameters": "13 Billion",
        "hub(base)": "project-baize/baize-v2-13b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/baize.yaml",
        "desc": "This model is a 13B Baize-v2, trained with supervised fine-tuning (SFT) and self-distillation with feedback (SDF). This checkpoint has been merged with LLaMA 7B so it's ready for use.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },    
    "vicuna-13b": {
        "category": "<20B",
        "display_name": "Vicuna",
        "thumb": "https://i.ibb.co/vqPDrPQ/vicuna.png",
        "parameters": "13 Billion",
        "hub(base)": "LLMs/Vicuna-13b-v1.1",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "Vicuna 13B is an open-source chatbot trained by fine-tuning LLaMA 13B on user-shared conversations collected from ShareGPT. It is an auto-regressive language model, based on the transformer architecture.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },    
    "gpt4-alpaca-13b": {
        "category": "<20B",
        "display_name": "GPT4-Alpaca-LoRA",
        "thumb": "https://i.ibb.co/qDz3HCG/chansung-vector-logo-of-alpaca-made-out-of-machines-Side-shot-39b27595-8202-48a6-97d1-266a745b2a29-r.png",
        "parameters": "13 Billion",
        "hub(base)": "elinas/llama-13b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/AlpacaGPT4-LoRA-13B-elina",
        "default_gen_config": "configs/response_configs/gpt4_alpaca.yaml",
        "desc": "GPT4 Alpaca LoRA is LLaMA 13B model fine-tuned on GPT4 generated instruction dataset.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },    
    "flan-11b": {
        "category": "<20B",
        "display_name": "Flan-XXL",
        "thumb": "https://i.ibb.co/yBTk5bv/flan.png",
        "parameters": "11 Billion",
        "hub(base)": "declare-lab/flan-alpaca-xxl",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/flan.yaml",
        "desc": "flan-alpaca-xxl is [Flan-T5-XXL](https://huggingface.co/google/flan-t5-xl) 11B fine-tuned on [Flan](https://github.com/google-research/FLAN) and [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) datasets.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "stable-vicuna-13b": {
        "category": "<20B",
        "display_name": "Stable-Vicuna",        
        "thumb": "https://i.ibb.co/b6Vv6Jh/sv.png",
        "parameters": "13 Billion",
        "hub(base)": "LLMs/Stable-Vicuna-13B",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/stablelm.yaml",
        "desc": "StableVicuna-13B is a [Vicuna-13B v0](https://huggingface.co/lmsys/vicuna-13b-delta-v0) model fine-tuned using reinforcement learning from human feedback (RLHF) via Proximal Policy Optimization (PPO) on various conversational and instructional datasets.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "camel-20b": {
        "category": "<30B",
        "display_name": "Camel",
        "thumb": "https://i.ibb.co/qD5HN9T/camel-removebg-preview.png",
        "parameters": "20 Billion",
        "hub(base)": "Writer/camel-20b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/camel.yaml",
        "desc": "Derived from the foundational architecture of [Palmyra-Base](https://huggingface.co/Writer/palmyra-base), Camel-20b is fine-tuned on an extensive dataset of approximately 70,000 instruction-response records. These records are generated by our dedicated [Writer](https://writer.com/) Linguist team, who possess considerable expertise in language modeling and fine-tuning techniques.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "starchat-15b": {
        "category": "<20B",
        "display_name": "StarChat",
        "thumb": "https://i.ibb.co/QjPP0Vv/starcoder.png",
        "parameters": "15.5 Billion",
        "hub(base)": "HuggingFaceH4/starchat-alpha",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/starchat.yaml",
        "desc": "StarChat is a series of language models that are fine-tuned from StarCoder to act as helpful coding assistants. StarChat Alpha is the first of these models, and as an alpha release is only intended for educational or research purpopses.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "llama-deus-7b": {
        "category": "<10B",
        "display_name": "LLaMA Deus",        
        "thumb": "https://i.ibb.co/4mH9LRQ/llama-deus.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "teknium/llama-deus-7b-v3-lora",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "LLaMA Deus is LLaMA 7B model fine-tuned on [GPTeacher](https://github.com/teknium1/GPTeacher)(General Instruct, Code Instruct, Roleplay Instruct, Roleplay V2 Instruct), GPT4-LLM Uncensored + Unnatural Instructions, [WizardLM Uncensored](https://huggingface.co/datasets/ehartford/wizard_vicuna_70k_unfiltered)",
        "example1": [
            ["Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.", "Hawaii was like stepping into a tropical paradise - vibrant colors, warm breezes, and lush greenery at every turn. From snorkeling in crystal clear waters to exploring ancient volcanoes and experiencing authentic Polynesian culture, there's no shortage of adventure here! Here's our guide to making your dream vacation to Hawaii a reality.\nAs soon as we touched down on Maui, we were struck by how different this island feels from anywhere else. The air is thick with palm trees and sweet fragrances of blooming flowers, and the sounds of traditional music can be heard wafting through the streets. One must-visit attraction is Iao Valley State Monument, which boasts stunning views of the West Maui Mountains. The site of ancient volcanic craters makes for a unique sight that you won't find anywhere else in the world.\nAnother great opportunity to immerse yourself in local traditions comes during Luau performances. These feasts often include hula dancers who tell tales through their moves while serving up delicious island cuisine such as pū‘ohe‘e (steamed crab legs) and kalua ‘āna (pig roast). After indulging in some exotic foods, visitors can experience firsthand what it means to dance the hula by taking part in one of many scheduled workshops available throughout the islands.\nA visit to Honolulu wouldn't be complete without seeing Pearl Harbor Memorial and National Cemetery. This historic site commemorates those who lost their lives during World War II and is accompanied by moving exhibits detailing events leading up to and following the attack. If you have the chance, consider taking a tour guided by a knowledgeable docent – they can offer incredible insights into the significance of the memorial. And remember to take time for reflection and respect for the sacrifices made by others.\nFor nature lovers, there are plenty of exciting activities to fill out your days including hiking through Haleakala Crater, Snorkelling along Molokini Crater, surfing and even camping within various state parks, all surrounded by breath-taking scenery. We also had the pleasure of driving across the roadless states to hike to the top of Pidiania Peake, where the summit provides panoramic views of neighboring islands. Overall, our journey provided enough excitement and tranquility to make us eager for our next visit. So pack your bags and don't miss out on the opportunity to explore these magical places we call home."]
        ],
        "example2": [
            ["Can you help me write a resignation letter to my current employer, while leaving on good terms and expressing gratitude for the opportunities provided?", "Dear [Employer],\nAs you know, I have been working at ABC Inc. for the past five years, where I have had the opportunity to learn and grow in my role as a software developer. Recently, I received another job offer from XYZ Corp. that I simply cannot pass up due to its location, culture, and career growth potential.\nWhile this decision wasn't easy to make, I am confident in saying that moving to XYZ Corp. will allow me to further develop my skills and reach new professional heights. It has truly been a pleasure working alongside you and the rest of the team at ABC Inc., and I would like to express my deepest gratitude for the opportunities provided.\nPlease accept this letter as my formal resignation effective two weeks from today. I look forward to celebrating our successes together and wishing you all the best in your future endeavors.\nThank you again for everything.\nSincerely,\n[Your Name]"]
        ],
        "example3": [
            ["What factors would you consider when designing an inclusive and accessible public transportation system?", "When designing an inclusive and accessible public transportation system, several factors should be considered. These include:\n1.Accessibility features - The system must have ramps, lifts, elevators, and other facilities that cater to passengers with disabilities such as wheelchairs, walkers, and crutches.\n2.Route planning - The routes must be planned to ensure easy access for all passengers, including those with physical limitations. This includes having stop signs at ground level rather than requiring users to navigate curbs or steps.\n3.Signage - Clear signage must be provided throughout the system to guide users from one place to another. All signs must meet international standards to make them easily understood by passengers with visual impairments.\n4.Information available - Passengers must be able to quickly and easily access information on schedules, fares, and route changes. This can be done through smart systems that allow customers to plan their journey digitally using real time updates.\n5.Staff training - All staff working within the transportation system must undergo extensive training to be sensitive to the needs of passengers with disabilities and how to assist them effectively.\n6.Customer feedback - Regularly collect feedback from passengers with disabilities to identify areas where improvements need to be made to the system. Use this input to continually enhance accessibility services."]
        ],
        "example4": [
            ["Write a python program to print the first 10 Fibonacci numbers", "Here's one possible implementation of a Python program that prints out the first ten Fibonacci numbers:\n```python\ndef fib(n):\n   if n <= 1:\n       return n\n   else:\n       return fib(n - 1) + fib(n - 2)\n   \ncount = 0\nfor i in range(fib_limit=9):\n   num = fib(i)\n   print(\"Fiboniculus No {}: {}\".format(count+1,num))\n\n\nfib_limit = int(input()) # Change this value from 9 to whatever you want the script to compute up until\n\nprint() # Add this line at the bottom to close the for loop properly"]
        ]
    },
    "evolinstruct-vicuna-7b": {
        "category": "<10B",
        "display_name": "EvolInstruct Vicuna",        
        "thumb": "https://i.ibb.co/xHDRjLS/evol-vicuna.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Vicuna-LoRA-EvolInstruct-7B",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "This is LLaMA 7B model fine-tuned on [WizardLM's EvolInstruct](https://github.com/nlpxucan/WizardLM) dataset re-organized into conversational format in Vicuna style.",
        "example1": [
            ["Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.", "Throughout my journey to Hawaii, I had the opportunity to experience and appreciate its unique culture in every step of the way. From visiting traditional temples to hiking through lush rainforests, this beautiful island offered much more than just beaches and sunshine. Here are some of the most unmissable cultural experiences that make up your must-do list for any future visits to Hawaii!\n\n1.Visit a Polynesian Cultural Center: The Polynesian Cultural Center is an outdoor museum featuring replicas of historic villages from across Polynesia. This place will give you a glimpse into the rich heritage and traditions of Pacific Islanders while learning how they live their daily lives. You can watch authentic dance performances, learn craft making skills like tattoo designs or bone carving, participate in hands-on activities, take a guided tour around the villages, dive deep into history and folklore stories. It was fascinating seeing people from all over the world come together to celebrate different cultures under one roof.\n\n2.Hike Through Haleakala National Park: One thing I really enjoyed doing on Maui was hiking through Haleakalā National Park. Not only did it provide breathtaking views of the volcanic crater but also a great chance to explore the diverse flora and fauna of the area including rare endemic species such as Silverswords and Lichenophrys johnstoni fern tree.\n\n3.Learn About History at Pearl Harbor Memorial: Honolulu’s famous Waikiki Beach may get all the attention these days, but not too far away lies an ominous yet significant site – PEARL HARBOR MEMORIAL. With its somber atmosphere, memorial, and exhibits commemorates the infamous Japanese attack on December 7th during WWII which led to America entry into World War II with honor. We visited the USS Arizona Memorial, which features a moving documentary film and boat ride along the battleship.\n\n4.Sample Local Cuisines - \"Plantation Village\": In ancient times, sugarcane plantations thrived throughout Hawaiian islands and became one of the primary drivers of economic growth. Today, visitors can experience a taste of those times by taking a stroll down Plantation Village located near the airport. Here, guests can browse various gift shops selling handmade goods made by locals, have fun riding mini trains or boats, and sample local cuisine from restaurants serving popular dishes like plate lunches (combination platter), chicken katsu sandwiches, kalua pork sliders, or poke bowls using fresh raw fish. These dining establishments serve as social hubs where people meet and enjoy each other’s company, sharing food and drinks while listening to music.\n\n5.Spend Time in Volcanoes National Park: Kīlauea volcano has been erupting constantly since January 2018 creating new landforms and attracting photographers who want to capture stunning scenery from vantage points above lava flows. But aside from witnessing nature at work, there are plenty of things to do inside the park as well. Guides lead treks to the summit of Mauna Loa, biological research stations offer educational programs, the Thomas A. Jaggar Museum showcases volcano science displays, geothermal wonders dot the landscape, and miles of trails cover varying terrain allowing you to hike, run, mountain bike, birdwatch, camp or simply soothe yourself watching a caldera lake surface boil.\n\nIn conclusion, Hawaii offers many unique cultural experiences beyond the expected resorts, beach bars, and luaus. Whether you choose to immerse yourself fully in Native culture or prefer spending time exploring natural beauty, this destination provides something for everyone."]
        ],
        "example2": [
            ["Can you help me write a resignation letter to my current employer, while leaving on good terms and expressing gratitude for the opportunities provided?", "Dear [Manager/CEO],\n\nAfter careful consideration, I am writing to inform you that I will be moving onto new professional challenges outside of our organization. This decision has not been made lightly, but it is one that I feel confident about as I look forward to pursuing greater career growth in another field.\n\nI would like to take this opportunity to thank you personally for providing me with countless learning experiences during my time at your company. Your guidance, mentorship, and leadership have helped shape who I am professionally today, and I cannot begin to express how much I appreciate those moments we shared together.\n\nFurthermore, I hope to convey the sincere sentiment of appreciation and gratitude for all the opportunities afforded to me within these walls. Your organization has offered me numerous chances to expand upon my skills set, gain valuable experience, and explore different roles within its function. It was always clear from day one that you valued diversity and encouraged personal growth among colleagues alike.\n\nWithout any hesitation or reservation, please accept this formal notice of my departure effective immediately. With regret, I must bid farewell to many meaningful relationships cultivated throughout these years. However, do know that my heartfelt wishes are with each individual involved here in achieving success beyond their wildest dreams. May fortune continue to smile down upon us!\n\nSincerely yours,\n\n[Your Name]"]
        ],
        "example3": [
            ["What factors would you consider when designing an inclusive and accessible public transportation system?", "1.Accessibility for people with disabilities - This includes measures such as wheelchair ramps on buses or trains, lifts in subway stations, and Braille signs indicating train platforms.\n2.Accommodations for pregnant women and mothers with young children - Comfortable seating areas, changing tables, and space for strollers are important accommodations that can make travel easier for families.\n3.Inclusivity for individuals of different ethnicities, religions, and sexual orientations - For example, ensuring non-gender specific bathrooms and designated parking spaces for those who need them.\n4.Consideration of age - Older adults may have mobility issues, so measures like low step entry points onto vehicles and handrails could be useful.\n5.Language barriers - Providing language assistance services or signage in multiple languages can help ensure all riders feel included."]
        ],
        "example4": [
            ["Write a python program to print the first 10 Fibonacci numbers", "```python\ndef fib(n): # recursive function that calculates the nth Fibonacci number\n   if n <= 2: return [0] + [1] * (len(fib) - n + len([x for x in fib[:-1]]))[::-1][:n+1]\nelse: return []\nfor i in range(10):\n   print(*fib(i), sep=' ')\nprint()\n```\nThis Python program defines a recursive function fib that computes each element of the sequence by adding the two preceding elements (a_n = b_{n}) until reaching the base cases b_2=1 or c_3 = c_2. The program then iterates through the first ten Fibonacci numbers using list comprehension syntax."]
        ]
    },
    "evolinstruct-vicuna-13b": {
        "category": "<20B",
        "display_name": "EvolInstruct Vicuna",        
        "thumb": "https://i.ibb.co/xHDRjLS/evol-vicuna.png",
        "parameters": "13 Billion",
        "hub(base)": "elinas/llama-13b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Vicuna-LoRA-EvolInstruct-13B",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "This is LLaMA 13B model fine-tuned on [WizardLM's EvolInstruct](https://github.com/nlpxucan/WizardLM) dataset re-organized into conversational format in Vicuna style.",
        "example1": [
            ["Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.", ""]
        ],
        "example2": [
            ["Can you help me write a resignation letter to my current employer, while leaving on good terms and expressing gratitude for the opportunities provided?", ""]
        ],
        "example3": [
            ["What factors would you consider when designing an inclusive and accessible public transportation system?", ""]
        ],
        "example4": [
            ["Write a python program to print the first 10 Fibonacci numbers", ""]
        ]
    },
    "alpacoom-7b": {
        "category": "<10B",
        "display_name": "Alpacoom",        
        "thumb": "https://huggingface.co/mrm8488/Alpacoom/resolve/main/alpacoom_logo__1___1___1_-removebg-preview.png",
        "parameters": "7 Billion",
        "hub(base)": "bigscience/bloom-7b1",
        "hub(ckpt)": "mrm8488/Alpacoom",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "This adapter was created with the PEFT library and allowed the base model [BigScience/BLOOM 7B1](https://huggingface.co/bigscience/bloom-7b1) to be fine-tuned on the Stanford's Alpaca Dataset by using the method LoRA.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "guanaco-7b": {
        "category": "<10B",
        "display_name": "Guanaco",        
        "thumb": "https://i.ibb.co/DWWsZn7/guanaco.png",
        "parameters": "7 Billion",
        "hub(base)": "decapoda-research/llama-7b-hf",
        "hub(ckpt)": "timdettmers/guanaco-7b",
        "default_gen_config": "configs/response_configs/guanaco.yaml",
        "desc": "LLaMA 7B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora) paper.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "guanaco-13b": {
        "category": "<20B",
        "display_name": "Guanaco",        
        "thumb": "https://i.ibb.co/DWWsZn7/guanaco.png",
        "parameters": "13 Billion",
        "hub(base)": "decapoda-research/llama-13b-hf",
        "hub(ckpt)": "timdettmers/guanaco-13b",
        "default_gen_config": "configs/response_configs/guanaco.yaml",
        "desc": "LLaMA 13B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora) paper.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "guanaco-33b": {
        "category": "<40B",
        "display_name": "Guanaco",        
        "thumb": "https://i.ibb.co/DWWsZn7/guanaco.png",
        "parameters": "33 Billion",
        "hub(base)": "timdettmers/guanaco-33b-merged",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/guanaco.yaml",
        "desc": "LLaMA 30B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora) paper.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "falcon-7b": {
        "category": "<10B",
        "display_name": "Falcon",
        "thumb": "https://i.ibb.co/86yNWwG/falcon.png",
        "parameters": "7 Billion",
        "hub(base)": "tiiuae/falcon-7b-instruct",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/falcon.yaml",
        "desc": "Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by [TII](https://www.tii.ae/) based on [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) and finetuned on a mixture of chat/instruct datasets including [Baize](https://github.com/project-baize/baize-chatbot), [GPT4All](https://github.com/nomic-ai/gpt4all), [GPTeacher](https://github.com/teknium1/GPTeacher), and [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)",
        "example1": [
            ["Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.", "I recently visited the beautiful Hawaiian island of Oahu and had the chance to experience both the local culture and some incredible attractions. One of my favorite experiences was visiting Pearl Harbor National Memorial Park, which was incredibly moving and provided insight into the history of the United States.\n\nOn the cultural side, I was lucky enough to attend a traditional luau and learn a lot about the island’s unique traditions and customs. The food was delicious, the dance performances were amazing, and I even learned a few new moves!\n\nFor outdoor adventure, I took a hike on the challenging Diamond Head Crater Trail, followed by a relaxing swim in the nearby ocean. And of course, no visit to Oahu would be complete without trying the famous shave ice for a sweet treat!\n\nOverall, my trip was unforgettable, and I look forward to returning to Hawaii in the future. If you're looking for an unforgettable cultural experience, I highly recommend visiting the island of Oahu."]
        ],
        "example2": [
            ["Can you help me write a resignation letter to my current employer, while leaving on good terms and expressing gratitude for the opportunities provided?", "Dear [Employer]\n\nThank you for giving me the opportunity to learn and grow over the past [time period]. I have been fortunate to have had the chance to work with such dedicated professionals and I have learned a great deal from each of you.\n\nI appreciate the challenges you've presented me with, as they have made me more adaptable and resourceful. I also appreciate the trust you have placed in me and the opportunities you've given me to learn and develop.\n\nI have been offered a new role and would like to accept it. I'm excited for the next journey in my career and would like to take this moment to thank you all for the support and guidance you have given me.\n\nSincerely,\n\n[Your Name]"]
        ],
        "example3": [
            ["What factors would you consider when designing an inclusive and accessible public transportation system?", "When designing an inclusive and accessible public transportation system, it is important to consider factors such as providing safe, reliable, and frequent service, ensuring that the design of the infrastructure accommodates the needs of different users, including those with disabilities, providing real-time updates to passengers, providing access to real-time route and schedule information, and offering customer service that is knowledgeable, friendly, and helpful. Additionally, factors such as environmental sustainability, affordability, and social equity should be considered to ensure that the system is accessible and welcoming for all.\n\nInclusivity and accessibility can be achieved by designing transportation systems that are safe, reliable, and efficient, while providing real-time updates and route and schedule information. It is also important to ensure that customer service is knowledgeable, friendly, and helpful."]
        ],
        "example4": [
            ["Write a python program to print the first 10 Fibonacci numbers", "The following is a Python program that prints the first 10 Fibonacci numbers:\n```\ndef fibonacci(n):\n    if n <= 10:\n        return n\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\nprint(fibonacci(10))\n```\nThis will output:\n```\n0\n1\n1\n2\n3\n5\n8\n13\n21\n34"]
        ]
    },
    "falcon-40b": {
        "category": "<40B",
        "display_name": "Falcon",
        "thumb": "https://i.ibb.co/86yNWwG/falcon.png",
        "parameters": "40 Billion",
        "hub(base)": "tiiuae/falcon-40b-instruct",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/falcon.yaml",
        "desc": "Falcon-7B-Instruct is a 40B parameters causal decoder-only model built by [TII](https://www.tii.ae/) based on [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) and finetuned on a mixture of chat/instruct datasets including [Baize](https://github.com/project-baize/baize-chatbot), [GPT4All](https://github.com/nomic-ai/gpt4all), [GPTeacher](https://github.com/teknium1/GPTeacher), and [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "wizard-falcon-7b": {
        "category": "<10B",
        "display_name": "Wizard Falcon",
        "thumb": "https://i.ibb.co/415s0D4/wizard-falcon.png",
        "parameters": "7 Billion",
        "hub(base)": "ehartford/WizardLM-Uncensored-Falcon-7b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/falcon.yaml",
        "desc": "This is WizardLM trained on top of tiiuae/falcon-7b, with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA. An uncensored model has no guardrails.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "wizard-falcon-40b": {
        "category": "<40B",
        "display_name": "Wizard Falcon",
        "thumb": "https://i.ibb.co/415s0D4/wizard-falcon.png",
        "parameters": "40 Billion",
        "hub(base)": "ehartford/WizardLM-Uncensored-Falcon-40b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/falcon.yaml",
        "desc": "This is WizardLM trained on top of tiiuae/falcon-40b, with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA. An uncensored model has no guardrails.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },    
    "nous-hermes-13b": {
        "category": "<20B",
        "display_name": "Nous Hermes",
        "thumb": "https://i.ibb.co/sm8VgtL/nous-hermes.png",
        "parameters": "13 Billion",
        "hub(base)": "NousResearch/Nous-Hermes-13b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "The model is LLaMA 13B that was fine-tuned almost entirely on synthetic GPT-4 outputs. This includes data from diverse sources such as GPTeacher, the general, roleplay v1&2, code instruct datasets, Nous Instruct & PDACTL (unpublished), CodeAlpaca, Evol_Instruct Uncensored, GPT4-LLM, and Unnatural Instructions. Additional data inputs came from Camel-AI's Biology/Physics/Chemistry and Math Datasets, Airoboros' GPT-4 Dataset, and more from CodeAlpaca. The total volume of data encompassed over 300,000 instructions.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]],
        "example4": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    }
}
