{
    "t5-vicuna-3b": {
        "category": "<10B",
        "display_name": "T5 Vicuna",
        "thumb": "https://i.ibb.co/4W7n78b/chansung-vector-logo-of-collective-intelligence-of-cute-llamas-3ef46884-72e6-44da-b88a-e831e5fee747.png",
        "parameters": "3 Billion",
        "hub(base)": "lmsys/fastchat-t5-3b-v1.0",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/t5_vicuna.yaml",
        "desc": "This model is based on [Flan-T5-XL](https://huggingface.co/google/flan-t5-xl) from google, and it is fine-tuned on [ShareGPT](https://sharegpt.com/) dataset curated by [LMSYS](https://lmsys.org/) in [Vicuna style](https://github.com/lm-sys/FastChat)",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "replit-3b": {
        "category": "<10B",
        "display_name": "Replit Instruct",
        "thumb": "https://i.ibb.co/BrKCKYq/replit.png",
        "parameters": "3 Billion",
        "hub(base)": "teknium/Replit-v1-CodeInstruct-3B",
        "hub(ckpt)": "N/A",
        "desc": ""
    },
    "camel-5b": {
        "category": "<10B",
        "display_name": "Camel",
        "thumb": "https://i.ibb.co/qD5HN9T/camel-removebg-preview.png",
        "parameters": "5 Billion",
        "hub(base)": "Writer/camel-5b-hf",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/camel.yaml",
        "desc": "Derived from the foundational architecture of [Palmyra-Base](https://huggingface.co/Writer/palmyra-base), Camel-5b is fine-tuned on an extensive dataset of approximately 70,000 instruction-response records. These records are generated by our dedicated [Writer](https://writer.com/) Linguist team, who possess considerable expertise in language modeling and fine-tuning techniques.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "mpt-7b": {
        "category": "<10B",
        "display_name": "MPT",
        "thumb": "https://i.ibb.co/DwN44Z9/mpt.png",
        "parameters": "7 Billion",
        "hub(base)": "mosaicml/mpt-7b-chat",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "[MPT-7B-Chat](https://huggingface.co/spaces/mosaicml/mpt-7b) is a chatbot-like model for dialogue generation. It was built by finetuning MPT-7B on the [ShareGPT-Vicuna](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna), [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3), [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca), [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf), and [Evol-Instruct](https://huggingface.co/datasets/victor123/evol_instruct_70k) datasets.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "redpajama-7b": {
        "category": "<10B",
        "display_name": "RedPajama",
        "thumb": "https://i.ibb.co/NNB6qPj/redpajama.png",
        "parameters": "7 Billion",
        "hub(base)": "togethercomputer/RedPajama-INCITE-Chat-7B-v0.1",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/redpajama.yaml",
        "desc": "RedPajama-INCITE-Chat-7B-v0.1 was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. It is fine-tuned on [OASST1](https://huggingface.co/datasets/OpenAssistant/oasst1) and [Dolly2](https://huggingface.co/datasets/databricks/databricks-dolly-15k) to enhance chatting ability.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },    
    "alpaca-lora-7b": {
        "category": "<10B",
        "display_name": "Alpaca-LoRA",
        "thumb": "https://i.ibb.co/z89FTz2/alpaca-lora.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Alpaca-LoRA-7B-elina",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "Alpaca LoRA is LLaMA 7B model fine-tuned on Alpaca dataset which is originally introduced by Standford. The dataset in this model was cleaned and re-generated by GPT4 by the community.",
        "example1": [
            ["Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.", "Aloha! Have you ever dreamed of exploring the beautiful tropical paradise that is Hawai'i? If so, then it’s time for your journey of discovery to begin! I recently had the privilege of embarking on an epic adventure through one of America’s most exciting destinations—and here are some of my favorite memories from the experience.\n\nFrom the stunning sights of Honolulu like Diamond Head Crater to the breathtaking beauty of Maui’s Wailea coastline, there were plenty of jaw-dropping moments during this journey across five islands in fourteen days. My top pick would have to be our visit to the island of Kauai, home to iconic natural landmarks such as the Napali Coast, which was truly spectacular. The Na Pali Cliffs stretch over two thousand feet above sea level while providing impressive views of the Pacific Ocean below. It felt almost otherworldly seeing these cliffs rise straight up out of the ocean; I can hardly believe they exist in reality at all! Other memorable stops included Oahu, where we toured Pearl Harbor; Molokai with its lush greenery, waterfalls, and untamed nature; Lana'i with its unique flora and fauna; and Haleakala National Park, where we marveled at incredible vistas of the mountain range. We even managed to take part in traditional luaus, hiking volcanos, and snorkeling amidst colorful fishes and corals - needless to say, each day brought us new surprises and fresh adventures. With its amazingly diverse landscape and culture, Hawai'i offers visitors something truly special. This remarkable place has become a treasured memory for me and will remain forever close to heart."]
        ],
        "example2": [
            ["Can you help me write a resignation letter to my current employer, while leaving on good terms and expressing gratitude for the opportunities provided?", "Dear [Name of Employer],\n\nIt is with both sadness and excitement that I am writing this email. After much thoughtful consideration over the last few months, I have decided to pursue other career pathways which better align with my skills and ambitions going forward. As such, it brings me great joy to inform you today that I will be moving onto new opportunities in two weeks time.\n\nI would like to take this opportunity to thank all those who work here at XYZ Co., as they made me feel welcome and valued from day one. It has been an absolute privilege to learn so many new things under your guidance during these past several years - including the importance of teamwork, the value of integrity and how to approach problems with creativity. The experiences gained through working together in our daily tasks have helped shape my professional journey and become more effective communicator throughout. For all of this I cannot truly find words to properly convey my appreciation.\n\nOn behalf of myself and family, we wish every success for yourself, the company and everyone involved in making us happy during these times. We look back fondly upon our days spent together, and know you’ll continue making positive impacts within this world far into the future. Thank you again for everything!"]
        ],
        "example3": [
            ["What factors would you consider when designing an inclusive and accessible public transportation system?", "When designing a public transportation network, it is important to take into account accessibility for all users. This includes ensuring that the route networks have wheelchair ramps or lifts so people with disabilities can use them without difficulty; providing adequate bus stops along the way which are equipped with signage that displays information such as departure times and routes in order to reduce confusion amongst passengers who need assistance boarding or de-boarding; employment of friendly staff trained on customer service skills; provision of audio announcements of destination at each stop so visually impaired persons can better orient themselves within their surroundings; offering discounts for elderly individuals or those with special needs; allowing passengers with reduced mobility space to move around freely inside the vehicle; etc"]
        ]
    },
    "alpaca-lora-13b": {
        "category": "<20B",
        "display_name": "Alpaca-LoRA",
        "thumb": "https://i.ibb.co/z89FTz2/alpaca-lora.png",
        "parameters": "13 Billion",
        "hub(base)": "elinas/llama-13b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Alpaca-LoRA-13B-elina",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "Alpaca LoRA is LLaMA 13B model fine-tuned on Alpaca dataset which is originally introduced by Standford. The dataset in this model was cleaned and re-generated by GPT4 by the community.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "koalpaca": {
        "category": "<20B",
        "display_name": "KoAlpaca",
        "thumb": "https://i.ibb.co/hF9NL7r/koalpaca.png",
        "parameters": "12.8 Billion",
        "hub(base)": "beomi/KoAlpaca-Polyglot-12.8B",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/koalpaca.yaml",
        "desc": "KoAlpaca is ko-polyglot 12.8B model fine-tuned on Alpaca dataset which is originally introduced by Standford. The dataset in this model was cleaned and translated into Korean using DeepL by the model creator.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "kullm": {
        "category": "<20B",
        "display_name": "kullm",
        "thumb": "https://i.ibb.co/6ZFqk4J/kullm.png",
        "parameters": "12.8 Billion",
        "hub(base)": "nlpai-lab/kullm-polyglot-12.8b-v2",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/koalpaca.yaml",
        "desc": "KoAlpaca is ko-polyglot 12.8B model fine-tuned on [GTP4ALL](https://github.com/nomic-ai/gpt4all), [Vicuna](https://github.com/lm-sys/FastChat), and [Dolly](https://github.com/databrickslabs/dolly) datasets. The datasets are translated into Korean using DeepL by the model creator.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },    
    "stablelm-7b": {
        "category": "<10B",
        "display_name": "StableLM",
        "thumb": "https://i.ibb.co/d2pd5wk/stable-LM-cropped.png",
        "parameters": "7 Billion",
        "hub(base)": "stabilityai/stablelm-tuned-alpha-7b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/stablelm.yaml",
        "desc": "StableLM 7B is a 7B parameter decoder-only language model built on top of the [StableLM-Base-Alpha](https://huggingface.co/stabilityai/stablelm-base-alpha-7b) model and further fine-tuned on various chat and instruction-following datasets including [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca), [GPT4ALL](https://huggingface.co/datasets/nomic-ai/gpt4all_prompt_generations), [Anthropic HH](https://huggingface.co/datasets/Dahoas/full-hh-rlhf), [Dolly](https://github.com/databrickslabs/dolly), and [ShareGPT](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna).",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "os-stablelm-7b": {
        "category": "<10B",
        "display_name": "OA+StableLM",
        "thumb": "https://i.ibb.co/WszrtVV/stablelm-oasst1.png",
        "parameters": "7 Billion",
        "hub(base)": "OpenAssistant/stablelm-7b-sft-v7-epoch-3",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/stablelm.yaml",
        "desc": "This is the 7th iteration English supervised-fine-tuning (SFT) model of the [Open-Assistant](https://github.com/LAION-AI/Open-Assistant) project. It is based on a StableLM 7B that was fine-tuned on human demonstrations of assistant conversations collected through the [https://open-assistant.io/](https://open-assistant.io/) human feedback web app before April 12, 2023. The datasets include [OASST](https://huggingface.co/datasets/OpenAssistant/oasst1), [Vicuna](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna), [Dolly](https://github.com/databrickslabs/dolly), grade school math instructions, and [code alpaca](https://github.com/sahil280114/codealpaca).",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "stackllama-7b": {
        "category": "<10B",
        "display_name": "StackLLaMA",
        "thumb": "https://i.ibb.co/Q9vLcYm/tuxpi-com-1682256296-removebg-preview.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "trl-lib/llama-7b-se-rl-peft",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": ""
    },
    "flan-3b": {
        "category": "<10B",
        "display_name": "Flan-XL",
        "thumb": "https://i.ibb.co/yBTk5bv/flan.png",
        "parameters": "3 Billion",
        "hub(base)": "declare-lab/flan-alpaca-xl",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/flan.yaml",
        "desc": "flan-alpaca-xl is [Flan-T5-XL](https://huggingface.co/google/flan-t5-xl) 3B fine-tuned on [Flan](https://github.com/google-research/FLAN) and [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) datasets.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "baize-7b": {
        "category": "<10B",
        "display_name": "Baize",
        "thumb": "https://i.ibb.co/j5VpHb0/baize.png",
        "parameters": "7 Billion",
        "hub(base)": "project-baize/baize-v2-7b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/baize.yaml",
        "desc": "This model is a 7B Baize-v2, trained with supervised fine-tuning (SFT) and self-distillation with feedback (SDF). This checkpoint has been merged with LLaMA 7B so it's ready for use.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "gpt4-alpaca-7b": {
        "category": "<10B",
        "display_name": "GPT4-Alpaca-LoRA",
        "thumb": "https://i.ibb.co/qDz3HCG/chansung-vector-logo-of-alpaca-made-out-of-machines-Side-shot-39b27595-8202-48a6-97d1-266a745b2a29-r.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/AlpacaGPT4-LoRA-7B-elina",
        "default_gen_config": "configs/response_configs/gpt4_alpaca.yaml",
        "desc": "GPT4 Alpaca LoRA is LLaMA 7B model fine-tuned on GPT4 generated instruction dataset.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "vicuna-7b": {
        "category": "<10B",
        "display_name": "Vicuna",
        "thumb": "https://i.ibb.co/vqPDrPQ/vicuna.png",
        "parameters": "7 Billion",
        "hub(base)": "LLMs/Vicuna-7b-v1.1",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "Vicuna 7B is an open-source chatbot trained by fine-tuning LLaMA 7B on user-shared conversations collected from ShareGPT. It is an auto-regressive language model, based on the transformer architecture.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "baize-13b": {
        "category": "<20B",
        "display_name": "Baize",
        "thumb": "https://i.ibb.co/j5VpHb0/baize.png",
        "parameters": "13 Billion",
        "hub(base)": "project-baize/baize-v2-13b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/baize.yaml",
        "desc": "This model is a 13B Baize-v2, trained with supervised fine-tuning (SFT) and self-distillation with feedback (SDF). This checkpoint has been merged with LLaMA 7B so it's ready for use.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },    
    "vicuna-13b": {
        "category": "<20B",
        "display_name": "Vicuna",
        "thumb": "https://i.ibb.co/vqPDrPQ/vicuna.png",
        "parameters": "13 Billion",
        "hub(base)": "LLMs/Vicuna-13b-v1.1",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "Vicuna 13B is an open-source chatbot trained by fine-tuning LLaMA 13B on user-shared conversations collected from ShareGPT. It is an auto-regressive language model, based on the transformer architecture.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },    
    "gpt4-alpaca-13b": {
        "category": "<20B",
        "display_name": "GPT4-Alpaca-LoRA",
        "thumb": "https://i.ibb.co/qDz3HCG/chansung-vector-logo-of-alpaca-made-out-of-machines-Side-shot-39b27595-8202-48a6-97d1-266a745b2a29-r.png",
        "parameters": "13 Billion",
        "hub(base)": "elinas/llama-13b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/AlpacaGPT4-LoRA-13B-elina",
        "default_gen_config": "configs/response_configs/gpt4_alpaca.yaml",
        "desc": "GPT4 Alpaca LoRA is LLaMA 13B model fine-tuned on GPT4 generated instruction dataset.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },    
    "flan-11b": {
        "category": "<20B",
        "display_name": "Flan-XXL",
        "thumb": "https://i.ibb.co/yBTk5bv/flan.png",
        "parameters": "11 Billion",
        "hub(base)": "declare-lab/flan-alpaca-xxl",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/flan.yaml",
        "desc": "flan-alpaca-xxl is [Flan-T5-XXL](https://huggingface.co/google/flan-t5-xl) 11B fine-tuned on [Flan](https://github.com/google-research/FLAN) and [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) datasets.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "stable-vicuna-13b": {
        "category": "<20B",
        "display_name": "Stable-Vicuna",        
        "thumb": "https://i.ibb.co/b6Vv6Jh/sv.png",
        "parameters": "13 Billion",
        "hub(base)": "LLMs/Stable-Vicuna-13B",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/stablelm.yaml",
        "desc": "StableVicuna-13B is a [Vicuna-13B v0](https://huggingface.co/lmsys/vicuna-13b-delta-v0) model fine-tuned using reinforcement learning from human feedback (RLHF) via Proximal Policy Optimization (PPO) on various conversational and instructional datasets.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "camel-20b": {
        "category": "<30B",
        "display_name": "Camel",
        "thumb": "https://i.ibb.co/qD5HN9T/camel-removebg-preview.png",
        "parameters": "20 Billion",
        "hub(base)": "Writer/camel-20b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/camel.yaml",
        "desc": "Derived from the foundational architecture of [Palmyra-Base](https://huggingface.co/Writer/palmyra-base), Camel-20b is fine-tuned on an extensive dataset of approximately 70,000 instruction-response records. These records are generated by our dedicated [Writer](https://writer.com/) Linguist team, who possess considerable expertise in language modeling and fine-tuning techniques.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "starchat-15b": {
        "category": "<20B",
        "display_name": "StarChat",
        "thumb": "https://i.ibb.co/QjPP0Vv/starcoder.png",
        "parameters": "15.5 Billion",
        "hub(base)": "HuggingFaceH4/starchat-alpha",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/starchat.yaml",
        "desc": "StarChat is a series of language models that are fine-tuned from StarCoder to act as helpful coding assistants. StarChat Alpha is the first of these models, and as an alpha release is only intended for educational or research purpopses.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "llama-deus-7b": {
        "category": "<10B",
        "display_name": "LLaMA Deus",        
        "thumb": "https://i.ibb.co/4mH9LRQ/llama-deus.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "teknium/llama-deus-7b-v3-lora",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "LLaMA Deus is LLaMA 7B model fine-tuned on [GPTeacher](https://github.com/teknium1/GPTeacher)(General Instruct, Code Instruct, Roleplay Instruct, Roleplay V2 Instruct), GPT4-LLM Uncensored + Unnatural Instructions, [WizardLM Uncensored](https://huggingface.co/datasets/ehartford/wizard_vicuna_70k_unfiltered)",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "evolinstruct-vicuna-7b": {
        "category": "<10B",
        "display_name": "EvolInstruct Vicuna",        
        "thumb": "https://i.ibb.co/xHDRjLS/evol-vicuna.png",
        "parameters": "7 Billion",
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Vicuna-LoRA-EvolInstruct-7B",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "This is LLaMA 7B model fine-tuned on [WizardLM's EvolInstruct](https://github.com/nlpxucan/WizardLM) dataset re-organized into conversational format in Vicuna style.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "evolinstruct-vicuna-13b": {
        "category": "<20B",
        "display_name": "EvolInstruct Vicuna",        
        "thumb": "https://i.ibb.co/xHDRjLS/evol-vicuna.png",
        "parameters": "13 Billion",
        "hub(base)": "elinas/llama-13b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Vicuna-LoRA-EvolInstruct-13B",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "This is LLaMA 13B model fine-tuned on [WizardLM's EvolInstruct](https://github.com/nlpxucan/WizardLM) dataset re-organized into conversational format in Vicuna style.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "alpacoom-7b": {
        "category": "<10B",
        "display_name": "Alpacoom",        
        "thumb": "https://huggingface.co/mrm8488/Alpacoom/resolve/main/alpacoom_logo__1___1___1_-removebg-preview.png",
        "parameters": "7 Billion",
        "hub(base)": "bigscience/bloom-7b1",
        "hub(ckpt)": "mrm8488/Alpacoom",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "This adapter was created with the PEFT library and allowed the base model [BigScience/BLOOM 7B1](https://huggingface.co/bigscience/bloom-7b1) to be fine-tuned on the Stanford's Alpaca Dataset by using the method LoRA.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "guanaco-7b": {
        "category": "<10B",
        "display_name": "Guanaco",        
        "thumb": "https://i.ibb.co/DWWsZn7/guanaco.png",
        "parameters": "7 Billion",
        "hub(base)": "decapoda-research/llama-7b-hf",
        "hub(ckpt)": "timdettmers/guanaco-7b",
        "default_gen_config": "configs/response_configs/guanaco.yaml",
        "desc": "LLaMA 7B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora) paper.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "guanaco-13b": {
        "category": "<20B",
        "display_name": "Guanaco",        
        "thumb": "https://i.ibb.co/DWWsZn7/guanaco.png",
        "parameters": "13 Billion",
        "hub(base)": "decapoda-research/llama-13b-hf",
        "hub(ckpt)": "timdettmers/guanaco-13b",
        "default_gen_config": "configs/response_configs/guanaco.yaml",
        "desc": "LLaMA 13B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora) paper.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "guanaco-33b": {
        "category": "<40B",
        "display_name": "Guanaco",        
        "thumb": "https://i.ibb.co/DWWsZn7/guanaco.png",
        "parameters": "33 Billion",
        "hub(base)": "timdettmers/guanaco-33b-merged",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/guanaco.yaml",
        "desc": "LLaMA 30B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora) paper.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "falcon-7b": {
        "category": "<10B",
        "display_name": "Falcon",
        "thumb": "https://i.ibb.co/86yNWwG/falcon.png",
        "parameters": "7 Billion",
        "hub(base)": "tiiuae/falcon-7b-instruct",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/falcon.yaml",
        "desc": "Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by [TII](https://www.tii.ae/) based on [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) and finetuned on a mixture of chat/instruct datasets including [Baize](https://github.com/project-baize/baize-chatbot), [GPT4All](https://github.com/nomic-ai/gpt4all), [GPTeacher](https://github.com/teknium1/GPTeacher), and [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "falcon-40b": {
        "category": "<40B",
        "display_name": "Falcon",
        "thumb": "https://i.ibb.co/86yNWwG/falcon.png",
        "parameters": "40 Billion",
        "hub(base)": "tiiuae/falcon-40b-instruct",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/falcon.yaml",
        "desc": "Falcon-7B-Instruct is a 40B parameters causal decoder-only model built by [TII](https://www.tii.ae/) based on [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) and finetuned on a mixture of chat/instruct datasets including [Baize](https://github.com/project-baize/baize-chatbot), [GPT4All](https://github.com/nomic-ai/gpt4all), [GPTeacher](https://github.com/teknium1/GPTeacher), and [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "wizard-falcon-7b": {
        "category": "<10B",
        "display_name": "Wizard Falcon",
        "thumb": "https://i.ibb.co/415s0D4/wizard-falcon.png",
        "parameters": "7 Billion",
        "hub(base)": "ehartford/WizardLM-Uncensored-Falcon-7b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/falcon.yaml",
        "desc": "This is WizardLM trained on top of tiiuae/falcon-7b, with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA. An uncensored model has no guardrails.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },
    "wizard-falcon-40b": {
        "category": "<40B",
        "display_name": "Wizard Falcon",
        "thumb": "https://i.ibb.co/415s0D4/wizard-falcon.png",
        "parameters": "40 Billion",
        "hub(base)": "ehartford/WizardLM-Uncensored-Falcon-40b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/falcon.yaml",
        "desc": "This is WizardLM trained on top of tiiuae/falcon-40b, with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA. An uncensored model has no guardrails.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    },    
    "nous-hermes-13b": {
        "category": "<20B",
        "display_name": "Nous Hermes",
        "thumb": "https://i.ibb.co/sm8VgtL/nous-hermes.png",
        "parameters": "13 Billion",
        "hub(base)": "NousResearch/Nous-Hermes-13b",
        "hub(ckpt)": "N/A",
        "default_gen_config": "configs/response_configs/default.yaml",
        "desc": "The model is LLaMA 13B that was fine-tuned almost entirely on synthetic GPT-4 outputs. This includes data from diverse sources such as GPTeacher, the general, roleplay v1&2, code instruct datasets, Nous Instruct & PDACTL (unpublished), CodeAlpaca, Evol_Instruct Uncensored, GPT4-LLM, and Unnatural Instructions. Additional data inputs came from Camel-AI's Biology/Physics/Chemistry and Math Datasets, Airoboros' GPT-4 Dataset, and more from CodeAlpaca. The total volume of data encompassed over 300,000 instructions.",
        "example1": [["hello #1", "world #1"], ["hello #2", "world #2"], ["hello #3", "world #3"]],
        "example2": [["hello #4", "world #4"], ["hello #5", "world #5"], ["hello #6", "world #6"]],
        "example3": [["hello #7", "world #7"], ["hello #8", "world #8"], ["hello #9", "world #9"]]
    }
}
